<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title> Playing Atari with Deep Reinforcement Learning —  &raquo;  Reading Club</title>
<meta name="description" content="Reading club rocks
">
<meta name="keywords" content="">
<link rel="canonical" href="/5weekplus/week5.html">
        <script src="https://code.jquery.com/jquery-3.1.0.min.js"   integrity="sha256-cCueBR6CsyA4/9szpPfrX3s49M9vUU5BgtiJj06wt/s="   crossorigin="anonymous"></script>

<script src="/assets/markdown.min.js"></script>

<!-- MathJax -->

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({ TeX: { extensions: ["color.js","cancel.js", "AMSmath.js", "AMSsymbols.js"] }});
  </script>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
    MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
      cancel: ["Extension","cancel"]
      });
   });
   </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
        inlineMath: [['$','$'], ['\\(','\\)']]
      }
    });
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      processEscapes: true
    }
  });
</script>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      Macros: {
        overlr: ['\\overset\\leftrightarrow{\#1}',1],
        overl: ['\\overset\leftarrow{\#1}',1],
        overr: ['\\overset\rightarrow{\#1}',1],
        bra: ['\\left\\langle \#1\\right|',1],
        ket: ['\\left| \#1\\right\\rangle',1],
        braket: ['\\langle \#1 \\mid \#2 \\rangle',2],
        avg: ['\\left< \#1 \\right>',1],
        slashed: ['\\cancel{\#1}',1],
        bold: ['\\boldsymbol{\#1}',1],
        sech: ['\\operatorname{sech}{\#1}',1],
        csch: ['\\operatorname{csch}{\#1}',1]
      }
    }
  });
  </script>

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

        <link rel="apple-touch-icon" sizes="57x57" href="/assets/favico/apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="/assets/favico/apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="/assets/favico/apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="/assets/favico/apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="/assets/favico/apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="/assets/favico/apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="/assets/favico/apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="/assets/favico/apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="/assets/favico/apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="/assets/favico/android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/favico/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="/assets/favico/favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/favico/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/assets/favico/ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

        




<!-- Twitter Cards -->
<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Playing Atari with Deep Reinforcement Learning" />
<meta name="twitter:description" content="Reading club rocks
" />
<meta name="twitter:image" content="" />

<!-- Google plus -->
<meta name="author" content="">
<link rel="author" href="">

<!-- Open Graph -->
<meta property="og:locale" content="">
<meta property="og:type" content="article">
<meta property="og:title" content="Playing Atari with Deep Reinforcement Learning">
<meta property="og:description" content="Reading club rocks
">
<meta property="og:url" content="/5weekplus/week5.html">
<meta property="og:site_name" content="Reading Club">

        <link href='//fonts.googleapis.com/css?family=Inconsolata:400,700' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="/assets/vendor/normalize-css/normalize.css">
<link rel="stylesheet" href="/css/main.css">

  <link rel="stylesheet" href="/assets/vendor/highlight/styles/solarized_dark.css">

<link rel="stylesheet" href="/assets/vendor/font-awesome/css/font-awesome.css">
<link rel="stylesheet" href="/assets/css/custom.css">
    </head>

    <body>
        <div class="wrapper">
            <header class="header">
    <div class="navigation">
        <a href="/" class="logo">Reading Club</a>

        <ul class="menu">
            
            <li class="menu__entry"><a href="/index.html">Events</a></li>
            
            <li class="menu__entry"><a href="/coordination">Coordination</a></li>
            
            <li class="menu__entry"><a href="/members">Members</a></li>
            
            <li class="menu__entry"><a href="/about">About</a></li>
            
            <!-- <li class="menu__entry"><a href="/coordination">Coordinate</a></li>
            <li class="menu__entry"><a href="/about">About</a></li> -->
            <!-- <li class="menu__entry"><a href="/">Blog</a></li> -->
        </ul>
    </div>

    <ul class="social-links">
        
        <a href="https://github.com/reading-club/reading-club.github.io/issues/new" class="social-links__entry" target="_blank">
                <i class="fa fa-envelope" aria-hidden="true"></i>
            </a>
        
        
            <a href="https://github.com/reading-club" class="social-links__entry" target="_blank">
                <i class="fa fa-github"></i>
            </a>
        

        
    </ul>
</header>


            <h1 class="page-title">
    <div class="page-title__text">Playing Atari with Deep Reinforcement Learning</div>
    <div class="page-title__subtitle"></div>
</h1>


<!-- <div style="text-align: center;border: 1px solid #000;padding: 1em;">
<span style="display:block;">Schedule: 2016-11-27 22:30:00, GMT+8 </span>
</div> -->


<div style="border-top: 1px solid #000;border-bottom: 1px solid #000;padding: 1em;">
<div class="post-teaser" style="margin-bottom: 0.5em !important;">
        <div class="post-teaser__infoblock">
        <span class="post-teaser__schedule">
				<i class="fa fa-calendar-o" aria-hidden="true"></i> Date: 2016-11-27 22:30:00, GMT+8</span>
        <br>
        <span class="post-teaser__speaker"><i class="fa fa-user" aria-hidden="true"></i> Speaker: zlslhp</span>
        <br>
        <span class="post-teaser__references"><i class="fa fa-paperclip" aria-hidden="true"></i> References: </span><br>
        
        <span class="post-teaser__reflist"><a href="https://arxiv.org/abs/1312.5602">https://arxiv.org/abs/1312.5602</a></span>
        
        </div>
    
</div>
</div>



<h1 id="playing-atari-with-deep-reinforcement-learning">Playing Atari with Deep Reinforcement Learning</h1>
<hr />

<h2 id="background">Background</h2>
<p>Could AI learn to control itself with the raw input from the environment, rather than receiving the hand-crafted features. In this article, the authors introduced an approach which combines reinforcement learning and deep neural network. This new deep learning model demonstrates its ability to master different control policies  for different Atari 2600 games with only raw pixel input.</p>

<h3 id="reinforcement-learning">Reinforcement Learning</h3>
<p>Future reward and its Bellman’s expression:
<script type="math/tex">R_{t}=\sum_{ {t}'=t}^{T }\gamma ^{ {t'}-t}r_{ {t}'}=r+\gamma R_{t^{'} }</script></p>

<p>The optimal action-value function:
<script type="math/tex">Q^{*}(s,a)=max_{\pi } \mathbb{E}[R_{t}|s_{t}=s,a_{t}=a,\pi]</script> 
This means the maximum expected return achievable, after seeing some sequence $s$ and then taking some action $a$ and by following policy $\pi$, which mapping sequence to actions.</p>

<p>The $Q^{<em>}(s,a)$ could be expressed using Bellman’s equation:
<script type="math/tex">Q^{*}(s,a)=\mathbb{E}_{s^{'}\sim  \varepsilon }[r+\gamma max_{ {a}'}Q^{*}({s}',{a}')|s,a]</script> 
However, $Q^{</em>}(s,a)$ is an optimal value. Normally, people use using the Bellman equation 
<script type="math/tex">Q_{i+1}(s,a)=\mathbb{E}[r+\gamma max_{ {a}'}Q_{i}({s}',{a}')|s,a]</script>
as an iterative update to converge to the optimal action-value function, when $Q_{i}\rightarrow Q^{*} $ as $i \rightarrow \infty $.</p>

<p>In this article, a neural network is used as a non-linear approximator to estimate the action-value function, $Q(s,a;\theta )\approx Q^{*}(s,a)$, where $\theta$ stands for the weights.
The loss function could be decribed as follows:
<script type="math/tex">L_{i}(\theta_{i})=\mathbb{E}_{s,a\sim  \rho (\cdot)}[(y_{i}-Q(s,a;\theta_{i}))^{2}]</script>
, where $y_{i}=\mathbb{E}<em>{ {s}’\sim \varepsilon }[r+\gamma max</em>{ {a}’}Q_{i}({s}’,{a}’;\theta_{i-1})|s,a]$ and $\rho(s,a)$ is a probability distribution over sequences $s$ and actions $a$.</p>

<h3 id="epsilon--greedy">$\epsilon $-greedy</h3>
<p>The next action is selected using $\epsilon $-greedy strategy.
<script type="math/tex">% <![CDATA[
\begin{cases}
a=\underset{a\in  A}{max}Q^{*}(s,a)& \text{with probability }1-\epsilon \\
\text{random action}& \text{with probability }\epsilon
\end{cases} %]]></script></p>

<h2 id="algorithm">Algorithm</h2>
<figure>
  <p><img src="/assets/posts/Playing_Atari_with_Deep_Reinforcement_Learning/algorithm.png" alt="" /></p>
  <figcaption>The algorithm for DQN training</figcaption>
</figure>

<h2 id="architecture">Architecture</h2>
<figure>
  <p><img src="/assets/posts/Playing_Atari_with_Deep_Reinforcement_Learning/arch.png" alt="" /></p>
  <figcaption>The architecture for DQN</figcaption>
</figure>

<h2 id="experiment-and-results">Experiment and Results</h2>
<figure>
  <p><img src="/assets/posts/Playing_Atari_with_Deep_Reinforcement_Learning/average_reward.png" alt="" /></p>
  <figcaption>The two plots on the left show average reward per episode on Breakout and Seaquest respectively during training. The statistics were computed by running an $\epsilon$-greedy policy with $\epsilon$ = 0.05 for 10000 steps. The two plots on the right show the average maximum predicted action-value of a held out set of states on Breakout and Seaquest respectively. One epoch corresponds to 50000 minibatch weight updates or roughly 30 minutes of training time.</figcaption>
</figure>

<figure>
  <p><img src="/assets/posts/Playing_Atari_with_Deep_Reinforcement_Learning/predicted_value.png" alt="" /></p>
  <figcaption>The leftmost plot shows the predicted value function for a 30 frame segment of the game Seaquest. The three screenshots correspond to the frames labeled by A, B, and C respectively.</figcaption>
</figure>

<figure>
  <p><img src="/assets/posts/Playing_Atari_with_Deep_Reinforcement_Learning/result.png" alt="" /></p>
  <figcaption>The upper table compares average total reward for various learning methods by running an $\epsilon$-greedy policy with $\epsilon =0.05$ for a fixed number of steps. The lower table reports results of the single best performing episode for HNeat and DQN. HNeat produces deterministic policies that always get the same score while DQN used an $\epsilon$-greedy policy with $\epsilon =0.05$.</figcaption>
</figure>




<span style="display:block;text-align:center;margin-top:5em;"><button class="js-gitter-toggle-chat-button btn"><i class="fa fa-comments" aria-hidden="true"></i> Toggle Chat </button></span>


<div class="explore">
    <div class="explore__devider">*****</div>
    <div class="explore__label">Explore other reading clubs</div>
    <ul class="categories">
            <li class="categories__item"><a href="https://github.com/neuronstar/spiking-neuron-models">Spiking Neuron Models</a></li>
		<li class="categories__item"><a href="/related-clubs.html">All</a></li>
    </ul>
</div>




<script>
  ((window.gitter = {}).chat = {}).options = {
    room: 'reading-club/Lobby',
    activationElement: false
  };
</script>
<script src="https://sidecar.gitter.im/dist/sidecar.v1.js" async defer></script>

        </div>

        <script src="/assets/vendor/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
        
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-81870366-1', 'auto');
      ga('send', 'pageview');
    </script>

    </body>
</html>
